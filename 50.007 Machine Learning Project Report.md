# 50.007 Machine Learning Project Report

*Seah Qi Yan 1004628* | *Ong Kah Yuan Joel 1004366*

[toc]

## Part 1: Labelling

Omitted.

## Part 2: Emission

The first order of business is to use maximum likelihood estimation to estimate the emission parameters from the training set, as per the equation below:
$$
e(x|y) = \frac{Count(y\rightarrow x)}{Count(y)}
$$
Our implementation of this can be found in the private method `__est_emission_params()` in the class part2. Here, we use a dictionary `count_y`, `count_y_to_x`, and `e_x_given_y` to keep track of the total number of occurrences of y, total number of emissions from y to x, and the probability of an emission of an x for a given y. We first populate `count_y_to_x` by iterating through the dataset, then iterating through that to populate `count_y`. Finally, `e_x_given_y` is generated as follows:

```python
#If the word token x:
for entry, count in count_y_to_x.items(): #where entry is a tuple (x,y)
	self.e_x_given_y[entry] = count/count_y[entry[1]]
    
#If the word token x is the special token #UNK#:
for entry, count in count_y.items(): #where entry is y
	self.e_x_given_y[("#UNK#",entry)] = 0.5/(count+0.5)

```

There is also the need to produce the tag:
$$
y^* = \underset{y}{\textrm{arg max }}e(x|y)
$$
for each word x in the sequence. This is done in the method `find_y_max_given_x`, which simply iterates through `e_x_given_y`, getting the value x that is associated with the highest probability for a given y, and associating the respective x and y as a key-value pair in the dictionary `x_max_prob`. Specifically, the code does the following:

```python
for entry, prob in self.e_x_given_y.items(): #where entry is a tuple (x,y)
    if entry[0] not in x_max_prob:
        x_max_prob[entry[0]] = prob
        self.y_max_given_x[entry[0]] = entry[1]
     else:
        if x_max_prob[entry[0]]<prob:
            x_max_prob[entry[0]] = prob
            self.y_max_given_x[entry[0]] = entry[1]
```

### Results

| Language  | Entity                                                | Sentiment                                                    |
| --------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| English   | Precision: 0.5996<br />Recall: 0.7240 <br />F: 0.5996 | Precision: 0.4461<br/>Recall: 0.6312<br/>Sentiment  F: 0.5227 |
| Chinese   | Precision: 0.0805<br/>Recall: 0.4886<br/>F: 0.1383    | Precision: 0.0381<br/>Recall: 0.2314<br/>F: 0.0655           |
| Singapore | Precision: 0.1926<br/>Recall: 0.5457<br/>F: 0.2847    | Precision: 0.1204<br/>Recall: 0.3413<br/>F: 0.1780           |

*Full results can be found in part_2_results.txt*

## Part 3: Transmission and Viterbi

We first need to estimate the transition parameters from the training set according to the following equation:
$$
q(y_i|y_{i-1})=\frac{Count(y_{i-1},y_i)}{Count(y_{i-1})}
$$
Our implementation (`est_transition_params` in the part3 class) uses...

We then need to use the Viterbi algorithm to compute the following:
$$
y_1^*,...,y_n^*=\underset{y_1,...,y_n}{\textrm{arg max }}p(x_1,...,x_n,y_1,...,y_n)
$$
In particular, we use the dynamic programming sequence:
$$
\pi(j+1,u) = \textrm{max}_v\{\pi(j,v)\times b_u(x_{j+1})\times a_{v,u}\} \\
\textrm{where: }\\
\begin{align}
\pi &= \textrm{a 2-D array}\\
u &= \textrm{current node}\\
v &= \textrm{previous node}\\
a &= \textrm{transition probability}\\
b &= \textrm{emission probarbility}
\end{align}
$$

### Results

| Language  | Entity                                             | Sentiment                                          |
| --------- | -------------------------------------------------- | -------------------------------------------------- |
| English   | Precision: 0.8318<br/>Recall: 0.8389<br/>F: 0.8354 | Precision: 0.7987<br/>Recall: 0.8055<br/>F: 0.8021 |
| Chinese   |                                                    |                                                    |
| Singapore | Precision: 0.6232<br/>Recall: 0.5092<br/>F: 0.5605 | Precision: 0.5356<br/>Recall: 0.4376<br/>F: 0.4816 |

*Full results can be found in part_3_results.txt*

## Part 4: Top 3 Sequences



## Part 5: Design Challenge


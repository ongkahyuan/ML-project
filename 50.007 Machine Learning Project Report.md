#  50.007 Machine Learning Project Report

*Seah Qi Yan 1004628*  |  *Ong Kah Yuan Joel 1004366*

[toc]

## Part 1: Labelling

Omitted.

## Part 2: Emission

The first order of business is to use maximum likelihood estimation to estimate the emission parameters from the training set, as per the equation below:
$$
e(x|y) = \frac{Count(y\rightarrow x)}{Count(y)}
$$
Our implementation of this can be found in the private method `__est_emission_params()` in the class part2. Here, we use a dictionary `count_y`, `count_y_to_x`, and `e_x_given_y` to keep track of the total number of occurrences of y, total number of emissions from y to x, and the probability of an emission of an x for a given y. We first populate `count_y_to_x` by iterating through the dataset, then iterating through that to populate `count_y`. Finally, `e_x_given_y` is generated as follows:

```python
#If the word token x:
for entry, count in count_y_to_x.items(): #where entry is a tuple (x,y)
	self.e_x_given_y[entry] = count/count_y[entry[1]]
    
#If the word token x is the special token #UNK#:
for entry, count in count_y.items(): #where entry is y
	self.e_x_given_y[("#UNK#",entry)] = 0.5/(count+0.5)

```

There is also the need to produce the tag:
$$
y^* = \underset{y}{\textrm{arg max }}e(x|y)
$$
for each word x in the sequence. This is done in the method `find_y_max_given_x`, which simply iterates through `e_x_given_y`, getting the value x that is associated with the highest probability for a given y, and associating the respective x and y as a key-value pair in the dictionary `x_max_prob`. Specifically, the code does the following:

```python
for entry, prob in self.e_x_given_y.items(): #where entry is a tuple (x,y)
    if entry[0] not in x_max_prob:
        x_max_prob[entry[0]] = prob
        self.y_max_given_x[entry[0]] = entry[1]
     else:
        if x_max_prob[entry[0]]<prob:
            x_max_prob[entry[0]] = prob
            self.y_max_given_x[entry[0]] = entry[1]
```

### Results

| Language  | Entity                                                | Sentiment                                                    |
| --------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| English   | Precision: 0.5996<br />Recall: 0.7240 <br />F: 0.5996 | Precision: 0.4461<br/>Recall: 0.6312<br/>Sentiment  F: 0.5227 |
| Chinese   | Precision: 0.0805<br/>Recall: 0.4886<br/>F: 0.1383    | Precision: 0.0381<br/>Recall: 0.2314<br/>F: 0.0655           |
| Singapore | Precision: 0.1926<br/>Recall: 0.5457<br/>F: 0.2847    | Precision: 0.1204<br/>Recall: 0.3413<br/>F: 0.1780           |

*Full results can be found in part_2_results.txt*

## Part 3: Transmission and Viterbi

We first need to estimate the transition parameters from the training set according to the following equation:
$$
q(y_i|y_{i-1})=\frac{Count(y_{i-1},y_i)}{Count(y_{i-1})}
$$
Our implementation (`est_transition_params` in the part3 class) uses...

We then need to use the Viterbi algorithm to compute the following:
$$
y_1^*,...,y_n^*=\underset{y_1,...,y_n}{\textrm{arg max }}p(x_1,...,x_n,y_1,...,y_n)
$$
In particular, we use the dynamic programming sequence:
$$
\pi(j+1,u) = \textrm{max}_v\{\pi(j,v)\times b_u(x_{j+1})\times a_{v,u}\} \\
\textrm{where: }\\
\begin{align}
\pi &= \textrm{a 2-D array}\\
u &= \textrm{current node}\\
v &= \textrm{previous node}\\
a &= \textrm{transition probability}\\
b &= \textrm{emission probarbility}
\end{align}
$$

### Results

| Language  | Entity                                             | Sentiment                                          |
| --------- | -------------------------------------------------- | -------------------------------------------------- |
| English   | Precision: 0.8318<br/>Recall: 0.8389<br/>F: 0.8354 | Precision: 0.7987<br/>Recall: 0.8055<br/>F: 0.8021 |
| Chinese   |                                                    |                                                    |
| Singapore | Precision: 0.6232<br/>Recall: 0.5092<br/>F: 0.5605 | Precision: 0.5356<br/>Recall: 0.4376<br/>F: 0.4816 |

*Full results can be found in part_3_results.txt*

## Part 4: Top 3 Sequences

In order to get the 3rd best sequence, there is a need to modify the original Viterbi algorithm. In particular, for the equation:
$$
\pi(j+1,u) = \textrm{max}_v\{\pi(j,v)\times b_u(x_{j+1})\times a_{v,u}\} \\
$$
There is a need to store the top 3 probabilities in $\pi(i,j)$ instead of just the highest one. Here, a 2D array fails to meet our requirements - we somehow need to maintain a sequence of probabilities and the previous state that led to it - and so in `part4.py` we use an object of our own implementation.

The class `queue` is essentially some modified priority queue concept. It maintains a series of dictionaries in an array, each of these dictionaries containing a `"p"` and `"previous"` keys to represent the probability and the previous state associated with it respectively. This array is sorted in descending order according to the value associated with the `"p"` key. 

The method `try_add(prob,state)` is the primary means of updating the queue. It first checks if `prob` is at least higher than the lowest probability currently in the queue. If it is, it deletes the dictionary associated with the lowest probability and adds the incoming `(prob, state)` as a dictionary. Upon which, the array holding the dictionaries is sorted. The implementation is as follows:

```python
class queue:
    ...
    def try_add(self,prob,state):
        if prob > self.min_prob or len(self.queue)<self.k:
            if len(self.queue)==self.k:
                del self.queue[-1]
            self.queue.append({"p":prob,"previous":state})
            self.queue = sorted(self.queue, key=lambda x:x["p"], reverse=True)
            ...
```

*The full implementation can be found in part4.py*

Viterbi then proceeds as per normal. After the full $\pi$ dictionary is built however, the back propagation sequence then extracts the state associated with the third best probability (instead of the first). The results of this implementation are as follows:

## Results

| Language | Entity                                             | Sentiment                                          |
| -------- | -------------------------------------------------- | -------------------------------------------------- |
| English  | Precision: 0.2904<br/>Recall: 0.4209<br/>F: 0.3437 | Precision: 0.0682<br/>Recall: 0.0989<br/>F: 0.0807 |

*Full results can be found in part_4_results.txt*

## Part 5: Design Challenge

